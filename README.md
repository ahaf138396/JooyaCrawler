
---

# ğŸ“Œ JooyaCrawler â€” High-Performance Asynchronous Web Crawler

**Jooya Search Engine â€“ MVP Core Crawler Module**
Built with **Python 3.12**, **AsyncIO**, **PostgreSQL**, **MongoDB**, **Tortoise ORM**, **HTTPX**, and fully containerized with **Docker Compose**

---

## ğŸ“– Overview

JooyaCrawler Ù‡Ø³ØªÙ‡ Ø§ØµÙ„ÛŒ Ø³ÛŒØ³ØªÙ… Ø®Ø²Ø´ Ø¯Ø± MVP Ù…ÙˆØªÙˆØ± Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ Ø¬ÙˆÛŒØ§ Ø§Ø³Øª.
Ø§ÛŒÙ† Ø³Ø±ÙˆÛŒØ³ Ø¨Ù‡ ØµÙˆØ±Øª **Ú©Ø§Ù…Ù„Ø§Ù‹ Async Ùˆ Event-Driven** Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ùˆ Ø´Ø§Ù…Ù„ Ø§Ø¬Ø²Ø§Ø¡ Ø²ÛŒØ± Ø§Ø³Øª:

* **Queue Management (PostgreSQL)**
* **Raw HTML Storage (MongoDB)**
* **Parallel Workers**
* **Periodic Scheduler**
* **HTML Parsing Engine (BeautifulSoup + Custom Logic)**
* **ORM Schema Auto-Initialization**
* **Dockerized Architecture**
* **High-scalability, fault-tolerant pipeline**

Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ù¾Ø§ÛŒÙ‡ Ø§ØµÙ„ÛŒ Ø®Ø²Ø´ØŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø§ÙˆÙ„ÛŒÙ‡ Ùˆ Ù‡Ø¯Ù Ù†Ù‡Ø§ÛŒÛŒ:
ğŸ”¥ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø§Ù…Ù„â€ŒØªØ±ÛŒÙ† Ù…ÙˆØªÙˆØ± Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ ÙØ§Ø±Ø³ÛŒ Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ Ø³Ø±ÙˆÛŒØ³ Ø®Ø§Ø±Ø¬ÛŒ.

---

# ğŸš€ Features

### âœ… **Asynchronous Architecture**

Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ `asyncio`ØŒ Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Ù‡Ø²Ø§Ø±Ø§Ù† Ø§ØªØµØ§Ù„ Ù‡Ù…Ø²Ù…Ø§Ù†.

### âœ… **PostgreSQL-Based Queue System**

Ù…Ø¯ÛŒØ±ÛŒØª ØµÙ URLs Ø¨Ø§:

* Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø¯ÙˆØ¨Ø§Ø±Ù‡â€ŒÚ©Ø§Ø±ÛŒ
* Atomic operations
* Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ pending / processing / done

### âœ… **MongoDB Storage**

Ø°Ø®ÛŒØ±Ù‡â€ŒÛŒ Ú©Ø§Ù…Ù„ HTMLØŒ headersØŒ status code Ùˆ Ù…ØªØ§Ø¯ÛŒØªØ§.

### âœ… **HTML Parsing Engine**

* Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù†
* Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ
* Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
* Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ú©Ø±Ø§ÙˆÙ„ external domains
* Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ

### âœ… **Parallel Workers**

Ø³Ù‡ Worker Ù¾ÛŒØ´â€ŒÙØ±Ø¶ (Ù‚Ø§Ø¨Ù„ Ø§ÙØ²Ø§ÛŒØ´):

* Download
* Save
* Extract Links
* Enqueue new tasks

### âœ… **Scheduler**

Ø§ÙØ²ÙˆØ¯Ù† Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ØŒ heartbeatØŒ Ø¢Ù…Ø§Ø± Ùˆ seed-refresh.

### âœ… **Auto Schema Initialization**

Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø¬Ø¯ÙˆÙ„â€ŒÙ‡Ø§ â†’ Ø®ÙˆØ¯Ú©Ø§Ø± Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.

### âœ… **Dockerized Setup**

Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø§ ÛŒÚ© Ø¯Ø³ØªÙˆØ±:

```
docker compose up -d
```

---

# ğŸ“‚ Project Structure

```
JooyaCrawler/
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ worker.py
â”‚   â”œâ”€â”€ parsing/
â”‚   â”‚   â””â”€â”€ html_extractor.py
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ page_model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ queue_model.py
â”‚   â”‚   â”œâ”€â”€ mongo/
â”‚   â”‚   â”‚   â”œâ”€â”€ mongo_manager.py
â”‚   â”‚   â”‚   â””â”€â”€ mongo_storage_manager.py
â”‚   â”‚   â””â”€â”€ postgres/
â”‚   â”‚       â”œâ”€â”€ postgres_init.py
â”‚   â”‚       â”œâ”€â”€ postgres_manager.py
â”‚   â”‚       â””â”€â”€ postgres_queue_manager.py
â”‚   â””â”€â”€ monitoring/
â”‚       â””â”€â”€ storage_monitor.py
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ›  Installation (Development)

### 1. Clone

```
git clone https://github.com/.../JooyaCrawler.git
cd JooyaCrawler
```

### 2. Install Dependencies

```
pip install -r requirements.txt
```

### 3. Configure Environment

Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ù…Ù‡Ù… Ø¯Ø± ÙØ§ÛŒÙ„ `.env` Ø±ÛŒØ´Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø´Ø§Ù…Ù„:

```
POSTGRES_USER=jooya
POSTGRES_PASSWORD=postgres
POSTGRES_DB=jooyacrawlerdb
DATABASE_URL=postgresql://jooya:postgres@postgres:5432/jooyacrawlerdb
MONGO_URI=mongodb://localhost:27017/jooyacrawlerdb
REDIS_URL=redis://localhost:6379/0
WORKERS=12
```

Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØºÛŒÛŒØ±ØŒ ÙØ§ÛŒÙ„ `.env` Ø±Ø§ ÙˆÛŒØ±Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ù…ØªØºÛŒØ±Ù‡Ø§ Ø±Ø§ Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¬Ø±Ø§ Ø³Øª Ù†Ù…Ø§ÛŒÛŒØ¯.

---

# ğŸ³ Run With Docker (Recommended)

```
docker compose up -d
```

### Check logs:

```
docker compose logs -f crawler
```

---

# âš™ï¸ Architecture Details

## Queue Manager (PostgreSQL)

* Ø°Ø®ÛŒØ±Ù‡ URL
* Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±
* atomic dequeue
* ÙˆØ¶Ø¹ÛŒØªâ€ŒÙ‡Ø§: pending / processing / done

## Worker Engine

* Ú¯Ø±ÙØªÙ† URL Ø§Ø² Queue
* Ø¯Ø§Ù†Ù„ÙˆØ¯ Async
* Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Mongo
* Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©
* Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙ
* Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø§Ù†Ø¬Ø§Ù…â€ŒØ´Ø¯Ù‡

## Scheduler

* Seed URL
* ØªØ²Ø±ÛŒÙ‚ Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ
* Ú©Ù†ØªØ±Ù„ Ø³Ø±Ø¹Øª Ùˆ Ø­Ø¬Ù… Ø¯ÛŒØªØ§Ø¨ÛŒØ³
* Ø¨Ú©â€ŒØ¢Ù Ù‡ÙˆØ´Ù…Ù†Ø¯

## Mongo Layer

* Ø¢Ø±Ø´ÛŒÙˆ HTML
* Ø§Ù…Ú©Ø§Ù† Ø°Ø®ÛŒØ±Ù‡ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù

---

# ğŸ§ª Example Log Output

```
[Worker-1] Crawled: https://example.com/page/23 (6342 bytes)
[Worker-1] Found and queued 182 links from https://example.com/page/23
Scheduler: Added https://example.com/page/24
PostgreSQL: Verified 2 tables
```

---

# ğŸ”’ Storage Monitoring (Coming Soon)

* Ù‡Ø´Ø¯Ø§Ø± Ø¯Ø±ØµÙˆØ±Øª Ø§ÙØ²Ø§ÛŒØ´ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
* Ú©Ø§Ù‡Ø´ Ø³Ø±Ø¹Øª crawl â†’ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù¾Ø± Ø´Ø¯Ù† Ø¯ÛŒØ³Ú©
* metrics Ø¨Ø±Ø§ÛŒ Prometheus

---

# ğŸ“ˆ Future Improvements

* Distributed crawling cluster
* URL canonicalization
* Duplicate content detection
* Robots.txt engine
* Sitemap crawler
* Anti-loop protection
* Rate-limit smoothing
* Crawling policies per domain
* Web-based dashboard

---

# ğŸ¤ Contributing

Pull Requests are welcome!

---

